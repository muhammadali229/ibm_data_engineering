{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e342475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, BooleanType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, struct, when, lit, sum, expr, array_contains, udf, upper, explode, row_number, rank, dense_rank, lead, current_date, date_format, to_date, datediff, from_json, to_json, json_tuple, get_json_object, collect_set, count, concat_ws, concat, monotonically_increasing_id, format_string, length\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark = SparkSession.builder.appName('comparision_v1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = my_spark.read.csv('./a.csv', header=True)\n",
    "df_b = my_spark.read.csv('./b.csv', header=True)\n",
    "WindowSpec = Window.orderBy('name', 'enroll')\n",
    "# df_a = df_a.withColumn('rn', row_number().over(WindowSpec))\n",
    "df_a = df_a.withColumn('rn', monotonically_increasing_id())\n",
    "# df_b = df_b.withColumn('rn', row_number().over(WindowSpec))\n",
    "df_b = df_b.withColumn('rn', monotonically_increasing_id())\n",
    "df_a = df_a.select([col(col_name).alias('a_'+col_name) for col_name in df_a.columns])\n",
    "df_b = df_b.select([col(col_name).alias('b_'+col_name) for col_name in df_b.columns])\n",
    "df_a_b = df_a.join(df_b, on=(df_a['a_rn'] == df_b['b_rn']), how='inner')\n",
    "df_a_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cols = list(zip(df_a.columns, df_b.columns))\n",
    "for ct in lst_cols:\n",
    "    df_a_b = df_a_b.withColumn(\n",
    "        f'cond_{ct[0]}_{ct[1]}',\n",
    "        when(\n",
    "            (col(ct[0]).isNull()),\n",
    "            concat(lit('Null Data -> Columns: '), lit(ct[0]), lit(' <> '),\n",
    "                   lit(ct[1]), lit(', Values: '), lit('null'), lit(' <> '),\n",
    "                   col(ct[1]), lit(', id: '), col('a_rn'))).when(\n",
    "                       (length(col(ct[1])) > length(col(ct[0]))) &\n",
    "                       (col(ct[1]).contains(col(ct[0]))),\n",
    "                       concat(lit('Precision Data -> Columns: '), lit(ct[0]),\n",
    "                              lit(' <> '), lit(ct[1]), lit(', Values: '),\n",
    "                              lit('null'), lit(' <> '), col(ct[1]),\n",
    "                              lit(', id: '), col('a_rn'))).when(\n",
    "                                  (col(ct[0]) != col(ct[1])),\n",
    "                                  concat(lit('Unmatched Data -> Columns: '),\n",
    "                                         lit(ct[0]), lit(' <> '), lit(ct[1]),\n",
    "                                         lit(', Values: '), col(ct[0]),\n",
    "                                         lit(' <> '), col(ct[1]),\n",
    "                                         lit(', id: '),\n",
    "                                         col('a_rn'))).otherwise(f'matched_e'))\n",
    "df_a_b = df_a_b.drop(*[i for i in df_a.columns + df_b.columns])\n",
    "df_a_b = df_a_b.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [j for col in df_a_b.columns for j in df_a_b[col] if j != 'matched_e']\n",
    "print('\\n'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c33671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, BooleanType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, struct, when, lit, sum, expr, array_contains, udf, upper, explode, row_number, rank, dense_rank, lead, current_date, date_format, to_date, datediff, from_json, to_json, json_tuple, get_json_object, collect_set\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "import time\n",
    "spark = SparkSession.builder.master('local[1]').appName('ali_spark_cond').getOrCreate()\n",
    "# spark.udf.registerJavaFunction('encrypt_sal', 'bt.encryptsal.EncryptSalary', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c85d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema).cache()\n",
    "# df.printSchema()\n",
    "df = df.filter(df['gender'] == 'M')\n",
    "df.write.csv(\"e3.csv\")\n",
    "# for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "#     rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029d5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Scan ExistingRDD[firstname#156,middlename#157,lastname#158,id#159,gender#160,salary#161]\n",
      " MapPartitionsRDD[24] at csv at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "    print(id, rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdc8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
