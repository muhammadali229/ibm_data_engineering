{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0fd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, BooleanType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, struct, when, lit, sum, expr, array_contains, udf, upper, explode, row_number, rank, dense_rank, lead, current_date, date_format, to_date, datediff, from_json, to_json, json_tuple, get_json_object, collect_set\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9fa369",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[1]').appName('ali_spark_cond').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4557e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|collect_set(age)|\n",
      "+----------------+\n",
      "|          [5, 2]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "df2.agg(collect_set('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a32ef8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[1]').appName('appv1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e657d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [\n",
    "    ('ali', 26),\n",
    "    ('haris', 25)\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be7eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('./aws_command_emr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e223107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "None\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', '', 'Smith', '1991-04-01', 'M', 3000),\n",
    "        ('Michael', 'Rose', '', '2000-05-19', 'M', 4000),\n",
    "        ('Robert', '', 'Williams', '1978-09-05', 'M', 4000),\n",
    "        ('Maria', 'Anne', 'Jones', '1967-12-01', 'F', 4000),\n",
    "        ('Jen', 'Mary', 'Brown', '1980-02-17', 'F', -1)]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(df.printSchema())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8080d413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|      cond_y|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|  [-1, 4000]|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|  [-1, 4000]|\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|[3000, 4000]|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|[3000, 4000]|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|[3000, 4000]|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    'cond_y',\n",
    "    collect_set('salary').over(Window.partitionBy('gender'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add5e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- coords: string (nullable = true)\n",
      " |-- start_year: string (nullable = true)\n",
      " |-- url_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- country_state: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+----------+--------------------+----------+----------+----------------+-------------+\n",
      "| id|      name|              coords|start_year|  url_name|         country|country_state|\n",
      "+---+----------+--------------------+----------+----------+----------------+-------------+\n",
      "|  5|  Aberdeen|  POINT(-2.15 57.15)|      2017|  aberdeen|        Scotland|         null|\n",
      "|  6|  Adelaide|POINT(138.6 -34.9...|      2017|  adelaide|       Australia|         null|\n",
      "|  7|   Algiers|POINT(3 36.83333333)|      2017|   algiers|         Algeria|         null|\n",
      "|  9|    Ankara|POINT(32.91666667...|      2017|    ankara|          Turkey|         null|\n",
      "| 16|     Belém|POINT(-48.4833333...|      2017|     belem|          Brazil|         null|\n",
      "| 10|  Asunción|POINT(-57.6666666...|      2017|  asuncion|        Paraguay|         null|\n",
      "| 11|    Athens|POINT(23.71666667...|      2017|    athens|          Greece|         null|\n",
      "| 12|  Auckland|POINT(174.75 -36....|      2017|  auckland|     New Zealand|         null|\n",
      "| 13|   Bangkok|  POINT(100.5 13.75)|      2017|   bangkok|        Thailand|         null|\n",
      "|211|    Ottawa|POINT(-75.7166666...|      2000|    ottawa|          Canada|         Ont.|\n",
      "| 17|   Belfast|POINT(-5.93333333...|      2017|   belfast|Northern Ireland|         null|\n",
      "| 18|  Belgrade|POINT(20.53333333...|      2017|  belgrade|          Serbia|         null|\n",
      "|273|St.-Brieuc|POINT(-2.78330326...|      2018|st.-brieuc|          France|         null|\n",
      "|274|   Poitier|POINT(0.333276529...|      2018|   poitier|          France|         null|\n",
      "|147|   Chicago|POINT(-87.6166666...|      1892|   chicago|   United States|         Ill.|\n",
      "|269|    Annecy|POINT(6.116670287...|      2018|    annecy|          France|         null|\n",
      "|270|    Roanne|POINT(4.066666218...|      2018|    roanne|          France|         null|\n",
      "|271|     Roura|POINT(-52.3300205...|      2018|     roura|          France|         null|\n",
      "|272| Sinnamary|POINT(-52.9599821...|      2018| sinnamary|          France|         null|\n",
      "|276|  Biarritz|POINT(-1.56159489...|      2018|  biarritz|          France|         null|\n",
      "+---+----------+--------------------+----------+----------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cities = spark.read.csv('./cities.csv', header=True)\n",
    "print(df_cities.printSchema())\n",
    "df_cities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ec6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.createOrReplaceTempView(\"CITIES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8871fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- coords: string (nullable = true)\n",
      " |-- start_year: string (nullable = true)\n",
      " |-- url_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- country_state: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+--------------------+----------+--------+---------+-------------+\n",
      "| id|    name|              coords|start_year|url_name|  country|country_state|\n",
      "+---+--------+--------------------+----------+--------+---------+-------------+\n",
      "|  5|Aberdeen|  POINT(-2.15 57.15)|      2017|aberdeen| Scotland|         null|\n",
      "|  6|Adelaide|POINT(138.6 -34.9...|      2017|adelaide|Australia|         null|\n",
      "|  7| Algiers|POINT(3 36.83333333)|      2017| algiers|  Algeria|         null|\n",
      "+---+--------+--------------------+----------+--------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql_cities = spark.sql('SELECT * FROM cities')\n",
    "print(df_sql_cities.printSchema())\n",
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f360cb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|      country|locations_count|\n",
      "+-------------+---------------+\n",
      "|United States|            115|\n",
      "|       France|             71|\n",
      "|       Canada|             14|\n",
      "|        Spain|              8|\n",
      "|      England|              8|\n",
      "+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        country, \n",
    "        count(*) as locations_count\n",
    "    from \n",
    "        cities \n",
    "    group by \n",
    "        country\n",
    "    order by\n",
    "        2 desc\n",
    "''').show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c547ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[102] at emptyRDD at NativeMethodAccessorImpl.java:0\n",
      "ParallelCollectionRDD[103] at parallelize at PythonRDD.scala:195\n"
     ]
    }
   ],
   "source": [
    "emptyRdd = spark.sparkContext.emptyRDD()\n",
    "emptyRddParal = spark.sparkContext.parallelize([])\n",
    "print(emptyRdd)\n",
    "print(emptyRddParal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d2860f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmptyRDD[105] at emptyRDD at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty rdd but with schema\n",
    "emptyRdd = spark.sparkContext.emptyRDD()\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType(), nullable=False),\n",
    "    StructField('Age', IntegerType(), nullable=False),\n",
    "    StructField('isMale', BooleanType(), nullable=False),\n",
    "])\n",
    "emptyRdd_students = spark.createDataFrame(emptyRdd, schema)\n",
    "emptyRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "053ec079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Age: integer (nullable = false)\n",
      " |-- isMale: boolean (nullable = false)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Age: integer (nullable = false)\n",
      " |-- isMale: boolean (nullable = false)\n",
      "\n",
      "None\n",
      "root\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# converting rdd to df\n",
    "emptyRdd = spark.sparkContext.emptyRDD()\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType(), nullable=False),\n",
    "    StructField('Age', IntegerType(), nullable=False),\n",
    "    StructField('isMale', BooleanType(), nullable=False),\n",
    "])\n",
    "df_1 = emptyRdd.toDF(schema)\n",
    "df_2 = spark.createDataFrame([], schema)\n",
    "df_2_with_empty_schema = spark.createDataFrame([], StructType([]))\n",
    "print(df_1.printSchema())\n",
    "print(df_2.printSchema())\n",
    "print(df_2_with_empty_schema.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38a25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Department: string (nullable = true)\n",
      " |-- no_of_employees: long (nullable = true)\n",
      "\n",
      "None\n",
      "+---------------------+---------------+\n",
      "|Department           |no_of_employees|\n",
      "+---------------------+---------------+\n",
      "|Business Intelligence|15             |\n",
      "|Data Engineering     |35             |\n",
      "|Software Engineering |62             |\n",
      "+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [\n",
    "    ('Business Intelligence', 15),\n",
    "    ('Data Engineering', 35),\n",
    "    ('Software Engineering', 62),\n",
    "]\n",
    "dept_df = spark.sparkContext.parallelize(dept)\n",
    "# dept_df.collect()\n",
    "df = dept_df.toDF(['Department', 'no_of_employees'])\n",
    "print(df.printSchema())\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fd7cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Department: string (nullable = true)\n",
      " |-- no_of_employees: long (nullable = true)\n",
      "\n",
      "None\n",
      "+---------------------+---------------+\n",
      "|Department           |no_of_employees|\n",
      "+---------------------+---------------+\n",
      "|Business Intelligence|15             |\n",
      "|Data Engineering     |35             |\n",
      "|Software Engineering |62             |\n",
      "+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [\n",
    "    ('Business Intelligence', 15),\n",
    "    ('Data Engineering', 35),\n",
    "    ('Software Engineering', 62),\n",
    "]\n",
    "dept_rdd = spark.sparkContext.parallelize(dept)\n",
    "dept_df = spark.createDataFrame(dept_rdd, schema=['Department', 'no_of_employees'])\n",
    "print(dept_df.printSchema())\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5b9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dept_name: string (nullable = false)\n",
      " |-- No_employees: integer (nullable = false)\n",
      "\n",
      "None\n",
      "+---------------------+------------+\n",
      "|Dept_name            |No_employees|\n",
      "+---------------------+------------+\n",
      "|Business Intelligence|15          |\n",
      "|Data Engineering     |35          |\n",
      "|Software Engineering |62          |\n",
      "+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [\n",
    "    ('Business Intelligence', 15),\n",
    "    ('Data Engineering', 35),\n",
    "    ('Software Engineering', 62),\n",
    "]\n",
    "dept_rdd = spark.sparkContext.parallelize(dept)\n",
    "schema = StructType([\n",
    "    StructField('Dept_name', StringType(), nullable=False),\n",
    "    StructField('No_employees', IntegerType(), nullable=False)\n",
    "])\n",
    "dept_df = spark.createDataFrame(dept_rdd, schema)\n",
    "print(dept_df.printSchema())\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d4f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td></td>\n",
       "      <td>Smith</td>\n",
       "      <td>36636</td>\n",
       "      <td>M</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Rose</td>\n",
       "      <td></td>\n",
       "      <td>40288</td>\n",
       "      <td>M</td>\n",
       "      <td>70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td></td>\n",
       "      <td>Williams</td>\n",
       "      <td>42114</td>\n",
       "      <td></td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Anne</td>\n",
       "      <td>Jones</td>\n",
       "      <td>39192</td>\n",
       "      <td>F</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Brown</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name middle_name last_name    dob gender  salary\n",
       "0      James                 Smith  36636      M   60000\n",
       "1    Michael        Rose            40288      M   70000\n",
       "2     Robert              Williams  42114         400000\n",
       "3      Maria        Anne     Jones  39192      F  500000\n",
       "4        Jen        Mary     Brown             F       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "employees_df = spark.createDataFrame(data, columns)\n",
    "print(employees_df.printSchema())\n",
    "# employees_df.show(truncate=False)\n",
    "employees_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9347c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- middle_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+-----+------+------+\n",
      "|Name                |dob  |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|[James, , Smith]    |36636|M     |3000  |\n",
      "|[Michael, Rose, ]   |40288|M     |4000  |\n",
      "|[Robert, , Williams]|42114|M     |4000  |\n",
      "|[Maria, Anne, Jones]|39192|F     |4000  |\n",
      "|[Jen, Mary, Brown]  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(James, , Smith)</td>\n",
       "      <td>36636</td>\n",
       "      <td>M</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Michael, Rose, )</td>\n",
       "      <td>40288</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Robert, , Williams)</td>\n",
       "      <td>42114</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Maria, Anne, Jones)</td>\n",
       "      <td>39192</td>\n",
       "      <td>F</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Jen, Mary, Brown)</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name    dob gender salary\n",
       "0      (James, , Smith)  36636      M   3000\n",
       "1     (Michael, Rose, )  40288      M   4000\n",
       "2  (Robert, , Williams)  42114      M   4000\n",
       "3  (Maria, Anne, Jones)  39192      F   4000\n",
       "4    (Jen, Mary, Brown)             F     -1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataStruct = [((\"James\", \"\", \"Smith\"), \"36636\", \"M\", \"3000\"),\n",
    "              ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", \"4000\"),\n",
    "              ((\"Robert\", \"\", \"Williams\"), \"42114\", \"M\", \"4000\"),\n",
    "              ((\"Maria\", \"Anne\", \"Jones\"), \"39192\", \"F\", \"4000\"),\n",
    "              ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", \"-1\")]\n",
    "schema = StructType([\n",
    "    StructField(\n",
    "        'Name',\n",
    "        StructType([\n",
    "            StructField('first_name', StringType()),\n",
    "            StructField('middle_name', StringType()),\n",
    "            StructField('last_name', StringType())\n",
    "        ])),\n",
    "    StructField(\n",
    "        'dob',\n",
    "        StringType(),\n",
    "    ),\n",
    "    StructField(\n",
    "        'gender',\n",
    "        StringType(),\n",
    "    ),\n",
    "    StructField(\n",
    "        'salary',\n",
    "        StringType(),\n",
    "    ),\n",
    "])\n",
    "employees_nested_df = spark.createDataFrame(dataStruct, schema)\n",
    "print(employees_nested_df.printSchema())\n",
    "employees_nested_df.show(truncate=False)\n",
    "employees_nested_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2004ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------\n",
      " Name   | [James, , Smith]  \n",
      " dob    | 36636             \n",
      " gender | M                 \n",
      " salary | 3000              \n",
      "-RECORD 1-------------------\n",
      " Name   | [Michael, Rose, ] \n",
      " dob    | 40288             \n",
      " gender | M                 \n",
      " salary | 4000              \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_nested_df.show(n=2, truncate=25, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ea176d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- middle_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structureData = [((\"Muhammad\", \"\", \"Ali\"), \"36636\", \"M\", 3100, ['swiming', 'gaming'], {'math':92, 'english': 32}),\n",
    "                 ((\"Johncena\", \"Uncle\", \"\"), \"40288\", \"M\", 4300, ['cricket'], {'english': 63}),\n",
    "                 ((\"Muhammad\", \"\", \"Talhah\"), \"42114\", \"M\", 1400, ['gaming'], {'science': 98}),\n",
    "                 ((\"Syeda\", \"Eraj\", \"Rizvi\"), \"39192\", \"F\", 5500, ['movies'], {'computer science': 92, 'math': 99}),\n",
    "                 ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", -1, ['--', 'na'], {'super': 120})]\n",
    "schema = StructType([\n",
    "    StructField(\n",
    "        'Name',\n",
    "        StructType([\n",
    "            StructField('first_name', StringType()),\n",
    "            StructField('middle_name', StringType()),\n",
    "            StructField('last_name', StringType())\n",
    "        ])),\n",
    "    StructField(\n",
    "        'id',\n",
    "        StringType(),\n",
    "    ),\n",
    "    StructField(\n",
    "        'gender',\n",
    "        StringType(),\n",
    "    ),\n",
    "    StructField(\n",
    "        'salary',\n",
    "        IntegerType(),\n",
    "    ),\n",
    "    StructField(\n",
    "        'hobbies',\n",
    "        ArrayType(StringType())\n",
    "    ),\n",
    "    StructField(\n",
    "        'properties',\n",
    "        MapType(StringType(), IntegerType())\n",
    "    )\n",
    "])\n",
    "employees_nested_df = spark.createDataFrame(structureData, schema)\n",
    "employees_nested_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d868b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- middle_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- moreinfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- salary: integer (nullable = true)\n",
      " |    |-- salary_grade: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_nested_df_updated_df = employees_nested_df.withColumn(\n",
    "    'moreinfo',\n",
    "    struct(\n",
    "        col('id').alias('identifier'),\n",
    "        col('gender').alias('gender'),\n",
    "        col('salary').alias('salary'),\n",
    "        when(col('salary').cast(IntegerType()) < 2000, 'low').when(\n",
    "            col('salary').cast(IntegerType()) < 4000,\n",
    "            'medium').otherwise('high').alias('salary_grade'))).drop(\n",
    "                'id', 'gender', 'salary')\n",
    "employees_nested_df_updated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99d9d022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>hobbies</th>\n",
       "      <th>properties</th>\n",
       "      <th>moreinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Muhammad, , Ali)</td>\n",
       "      <td>[swiming, gaming]</td>\n",
       "      <td>{'english': 32, 'math': 92}</td>\n",
       "      <td>(36636, M, 3100, medium)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Johncena, Uncle, )</td>\n",
       "      <td>[cricket]</td>\n",
       "      <td>{'english': 63}</td>\n",
       "      <td>(40288, M, 4300, high)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Muhammad, , Talhah)</td>\n",
       "      <td>[gaming]</td>\n",
       "      <td>{'science': 98}</td>\n",
       "      <td>(42114, M, 1400, low)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Syeda, Eraj, Rizvi)</td>\n",
       "      <td>[movies]</td>\n",
       "      <td>{'computer science': 92, 'math': 99}</td>\n",
       "      <td>(39192, F, 5500, high)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Jen, Mary, Brown)</td>\n",
       "      <td>[--, na]</td>\n",
       "      <td>{'super': 120}</td>\n",
       "      <td>(, F, -1, low)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name            hobbies  \\\n",
       "0     (Muhammad, , Ali)  [swiming, gaming]   \n",
       "1   (Johncena, Uncle, )          [cricket]   \n",
       "2  (Muhammad, , Talhah)           [gaming]   \n",
       "3  (Syeda, Eraj, Rizvi)           [movies]   \n",
       "4    (Jen, Mary, Brown)           [--, na]   \n",
       "\n",
       "                             properties                  moreinfo  \n",
       "0           {'english': 32, 'math': 92}  (36636, M, 3100, medium)  \n",
       "1                       {'english': 63}    (40288, M, 4300, high)  \n",
       "2                       {'science': 98}     (42114, M, 1400, low)  \n",
       "3  {'computer science': 92, 'math': 99}    (39192, F, 5500, high)  \n",
       "4                        {'super': 120}            (, F, -1, low)  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees_nested_df_updated_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47d9100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'mynameisali'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colobj = lit('mynameisali')\n",
    "colobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d19aa896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    26|\n",
      "|    31|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('ali', 26), ('faisal', 31)]\n",
    "df = spark.createDataFrame(data).toDF('name', 'gender')\n",
    "# df.printSchema()\n",
    "df.select(df.gender).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a078ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|   ali|\n",
      "|faisal|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "914969d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "| name|         prop|\n",
      "+-----+-------------+\n",
      "|James|[blue, black]|\n",
      "|  Ann|[black, grey]|\n",
      "+-----+-------------+\n",
      "\n",
      "+-----+-----+\n",
      "| name|  eye|\n",
      "+-----+-----+\n",
      "|James| blue|\n",
      "|  Ann|black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(name=\"James\", prop=Row(hair=\"black\", eye=\"blue\")),\n",
    "    Row(name=\"Ann\", prop=Row(hair=\"grey\", eye=\"black\"))\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "df.select(col('name'), col('prop.eye')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19c2cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|subject|marks|\n",
      "+-------+-----+\n",
      "|   math|   92|\n",
      "|english|   98|\n",
      "+-------+-----+\n",
      "\n",
      "+----------+\n",
      "|sum(marks)|\n",
      "+----------+\n",
      "|       190|\n",
      "+----------+\n",
      "\n",
      "+-----+\n",
      "|iam90|\n",
      "+-----+\n",
      "|false|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('math', 92), ('english', 98)]\n",
    "df = spark.createDataFrame(data, ['subject','marks'])\n",
    "df.show()\n",
    "df.select(sum(col('marks'))).show()\n",
    "df.select((sum(col('marks')) == 90).alias('iam90')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bbb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",None, None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F', 'Check'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "# df.select(expr(\"fname || ' ' || lname\").alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6082ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     fname| id|\n",
      "+----------+---+\n",
      "|     James|100|\n",
      "|       Ann|200|\n",
      "|Tom Cruise|400|\n",
      "| Tom Brand|400|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('fname'), col('id')).sort(col('id').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7fd2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('fname'), col('id').cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77d90c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+\n",
      "|     fname|lname| id|\n",
      "+----------+-----+---+\n",
      "|       Ann|Varsa|200|\n",
      "|Tom Cruise|  XXX|400|\n",
      "| Tom Brand| null|400|\n",
      "+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you don't specify cast it will automatically convert into integer explicitly\n",
    "df.select(col('fname'), col('lname'), col('id')).filter(col('id').cast('int').between(200, 500)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bb6afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     full_name|\n",
      "+--------------+\n",
      "|Tom Cruise XXX|\n",
      "|    Tom Brand |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"fname || ' ' || coalesce(lname, '')\").alias('full_name')).filter(col('fname').contains('Tom')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd35ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+------+\n",
      "|    fname|lname| id|gender|\n",
      "+---------+-----+---+------+\n",
      "|Tom Brand| null|400|     M|\n",
      "+---------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('lname').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41e7e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('lname').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c4a9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  null|\n",
      "+-----+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.fname.like(\"%es\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e2ed766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|     fname|lname|genderkanayacolumn|\n",
      "+----------+-----+------------------+\n",
      "|     James| Bond|              null|\n",
      "|       Ann|Varsa|            Female|\n",
      "|Tom Cruise|  XXX|                  |\n",
      "| Tom Brand| null|              Male|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('fname'), col('lname'),\n",
    "    when(col('gender') == 'M',\n",
    "         'Male').when(col('gender') == 'F', 'Female').otherwise(\n",
    "             col('gender')).alias('genderkanayacolumn')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "503edb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|  Ann|Varsa|200|     F|\n",
      "+-----+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.id.isin([10, 200])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71642dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961962ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name').getField('fname')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc42644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|properties[eye]|\n",
      "+---------------+\n",
      "|          brown|\n",
      "|          black|\n",
      "|           grey|\n",
      "|           blue|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('properties').getField('eye')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618b7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|properties[eye]|\n",
      "+---------------+\n",
      "|          brown|\n",
      "|          black|\n",
      "|           grey|\n",
      "|           blue|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('properties').getItem('eye')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97c9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|languages[1]|\n",
      "+------------+\n",
      "|          C#|\n",
      "|      Python|\n",
      "|       Scala|\n",
      "|        Ruby|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('languages').getItem(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22939e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------------+\n",
      "|          name|      languages|          properties|\n",
      "+--------------+---------------+--------------------+\n",
      "| [James, Bond]|     [Java, C#]|[eye -> brown, ha...|\n",
      "|  [Ann, Varsa]| [.NET, Python]|[eye -> black, ha...|\n",
      "|[Tom Cruise, ]|[Python, Scala]|[eye -> grey, hai...|\n",
      "|  [Tom Brand,]|   [Perl, Ruby]|[eye -> blue, hai...|\n",
      "+--------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['name', 'languages', 'properties']\n",
    "df.select(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a383569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          name|\n",
      "+--------------+\n",
      "| [James, Bond]|\n",
      "|  [Ann, Varsa]|\n",
      "|[Tom Cruise, ]|\n",
      "|  [Tom Brand,]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([i for i in columns if i == 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28785448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------------+\n",
      "|          name|      languages|          properties|\n",
      "+--------------+---------------+--------------------+\n",
      "| [James, Bond]|     [Java, C#]|[eye -> brown, ha...|\n",
      "|  [Ann, Varsa]| [.NET, Python]|[eye -> black, ha...|\n",
      "|[Tom Cruise, ]|[Python, Scala]|[eye -> grey, hai...|\n",
      "|  [Tom Brand,]|   [Perl, Ruby]|[eye -> blue, hai...|\n",
      "+--------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2d07f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      languages|\n",
      "+---------------+\n",
      "|     [Java, C#]|\n",
      "| [.NET, Python]|\n",
      "|[Python, Scala]|\n",
      "|   [Perl, Ruby]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[1:2]).show() # only show languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aefdf844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dept_name='Finance', dept_id=10),\n",
       " Row(dept_name='Marketing', dept_id=20),\n",
       " Row(dept_name='Sales', dept_id=30),\n",
       " Row(dept_name='IT', dept_id=40)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)\n",
    "deptDF.collect() # collect data from all nodes to driver node but used for smaller datasets, kindly avoid on larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cc613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6634760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('salary', col('salary').cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a7456f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M| 300.0|\n",
      "|  Michael|      Rose|        |2000-05-19|     M| 400.0|\n",
      "|   Robert|          |Williams|1978-09-05|     M| 400.0|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F| 400.0|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|  -0.1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('salary', col('salary') / 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a1346ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|eobi|   pf|\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000| 250|150.0|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000| 250|200.0|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000| 250|200.0|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000| 250|200.0|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1| 250|-0.05|\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('pf', col('salary') * 0.05)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f02db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|eobi|   pf|\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|  50|150.0|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|  50|200.0|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|  50|200.0|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|  50|200.0|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|  50|-0.05|\n",
      "+---------+----------+--------+----------+------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('eobi', lit(50))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e236de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+---+------+----+-----+\n",
      "|firstname|middlename|lastname|       dob|sex|salary|eobi|   pf|\n",
      "+---------+----------+--------+----------+---+------+----+-----+\n",
      "|    James|          |   Smith|1991-04-01|  M|  3000|  50|150.0|\n",
      "|  Michael|      Rose|        |2000-05-19|  M|  4000|  50|200.0|\n",
      "|   Robert|          |Williams|1978-09-05|  M|  4000|  50|200.0|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|  F|  4000|  50|200.0|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|  F|    -1|  50|-0.05|\n",
      "+---------+----------+--------+----------+---+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('gender', 'sex')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61bc8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+---+----+\n",
      "|firstname|lastname|       dob|sex|eobi|\n",
      "+---------+--------+----------+---+----+\n",
      "|    James|   Smith|1991-04-01|  M|  50|\n",
      "|  Michael|        |2000-05-19|  M|  50|\n",
      "|   Robert|Williams|1978-09-05|  M|  50|\n",
      "|    Maria|   Jones|1967-12-01|  F|  50|\n",
      "|      Jen|   Brown|1980-02-17|  F|  50|\n",
      "+---------+--------+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('middlename', 'salary', 'pf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33219aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5784ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+------+\n",
      "|                name|Date of Birth|gender|salary|\n",
      "+--------------------+-------------+------+------+\n",
      "|    [James, , Smith]|   1991-04-01|     M|  3000|\n",
      "|   [Michael, Rose, ]|   2000-05-19|     M|  4000|\n",
      "|[Robert, , Williams]|   1978-09-05|     M|  4000|\n",
      "|[Maria, Anne, Jones]|   1967-12-01|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|   1980-02-17|     F|    -1|\n",
      "+--------------------+-------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('dob', 'Date of Birth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0fa2268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+-------------+\n",
      "|                name|Date of Birth|gender|salary_amount|\n",
      "+--------------------+-------------+------+-------------+\n",
      "|    [James, , Smith]|   1991-04-01|     M|         3000|\n",
      "|   [Michael, Rose, ]|   2000-05-19|     M|         4000|\n",
      "|[Robert, , Williams]|   1978-09-05|     M|         4000|\n",
      "|[Maria, Anne, Jones]|   1967-12-01|     F|         4000|\n",
      "|  [Jen, Mary, Brown]|   1980-02-17|     F|           -1|\n",
      "+--------------------+-------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('dob', 'Date of Birth').withColumnRenamed('salary', 'salary_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3150796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+------+-------+\n",
      "|                name|       dob|gender|salary|  fname|\n",
      "+--------------------+----------+------+------+-------+\n",
      "|    [James, , Smith]|1991-04-01|     M|  3000|  James|\n",
      "|   [Michael, Rose, ]|2000-05-19|     M|  4000|Michael|\n",
      "|[Robert, , Williams]|1978-09-05|     M|  4000| Robert|\n",
      "|[Maria, Anne, Jones]|1967-12-01|     F|  4000|  Maria|\n",
      "|  [Jen, Mary, Brown]|1980-02-17|     F|    -1|    Jen|\n",
      "+--------------------+----------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*', col('name.firstname').alias('fname')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b4273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
      "|[Anna, Rose, ]        |[Spark, Java, C++]|NY   |F     |\n",
      "|[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |\n",
      "|[Maria, Anne, Jones]  |[CSharp, VB]      |NY   |M     |\n",
      "|[Jen, Mary, Brown]    |[CSharp, VB]      |NY   |M     |\n",
      "|[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57000baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state == 'NY').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df543f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|            name|         languages|state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|[James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|  [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(array_contains('languages', 'Java')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c435c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|   [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "| [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.firstname.contains('J')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4cce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461b57f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b7c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66c37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|          Jen|   Finance|  3900|\n",
      "|      Michael|     Sales|  4600|\n",
      "|        Scott|   Finance|  3300|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|         Saif|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e28534ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|department|salary|\n",
      "+----------+------+\n",
      "|     Sales|  4600|\n",
      "|     Sales|  4100|\n",
      "|   Finance|  3900|\n",
      "|   Finance|  3000|\n",
      "|   Finance|  3300|\n",
      "| Marketing|  2000|\n",
      "|     Sales|  3000|\n",
      "| Marketing|  3000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(['department', 'salary']).select('department', 'salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c7a07e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffee1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('salary', 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4be1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.salary.desc()).show() # Raman CIVIC WALA LONDA PAISA BOHAT HY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e0f2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('salary'), col('age').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da4f42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('Employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ab2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|department|no_employees|\n",
      "+----------+------------+\n",
      "|   Finance|           4|\n",
      "|     Sales|           3|\n",
      "| Marketing|           2|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT \n",
    "        department, \n",
    "        count(*) no_employees \n",
    "    FROM\n",
    "        employees\n",
    "    GROUP BY\n",
    "        department\n",
    "    ORDER BY\n",
    "        2 DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7efd9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50b9904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     257000|\n",
      "|   Finance|     351000|\n",
      "| Marketing|     171000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e5a82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').avg('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8335d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').mean('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6db59343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+\n",
      "|department|      avg(salary)|        avg(bonus)|\n",
      "+----------+-----------------+------------------+\n",
      "|     Sales|85666.66666666667|17666.666666666668|\n",
      "|   Finance|          87750.0|           20250.0|\n",
      "| Marketing|          85500.0|           19500.0|\n",
      "+----------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').mean('salary', 'bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7e5e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|department|salary|bonus|\n",
      "+----------+------+-----+\n",
      "|   Finance|351000|81000|\n",
      "|     Sales|257000|53000|\n",
      "| Marketing|171000|39000|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(\n",
    "    sum('salary').alias('salary'),\n",
    "    sum('bonus').alias('bonus')).orderBy(col('salary').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85da61b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|department|salary|bonus|\n",
      "+----------+------+-----+\n",
      "|   Finance|351000|81000|\n",
      "|     Sales|257000|53000|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(\n",
    "    sum('salary').alias('salary'),\n",
    "    sum('bonus').alias('bonus')).where(col('salary') > 200000).orderBy(\n",
    "        col('salary').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7c4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8dd4301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id).show(truncate=False) # by default inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980d9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id, how='outer').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3c1cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id, how='left').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3bddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id, how='right').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd231a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take inner join, select only left side columns\n",
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id, how='leftsemi').show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c88d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take left join but where right table have null in it's column\n",
    "empDF.join(deptDF, on=empDF.emp_dept_id == deptDF.dept_id, how='leftanti').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a413c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+-------------+\n",
      "|Emp_id|Emp_name|Superior_id|Superior_name|\n",
      "+------+--------+-----------+-------------+\n",
      "|1     |Smith   |1          |Rose         |\n",
      "|1     |Smith   |1          |Williams     |\n",
      "|2     |Rose    |2          |Jones        |\n",
      "|2     |Rose    |2          |Brown        |\n",
      "|2     |Rose    |2          |Brown        |\n",
      "+------+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.alias('emp1').join(\n",
    "    empDF.alias('emp2'),\n",
    "    on=col('emp1.emp_id') == col('emp2.superior_emp_id')).select(\n",
    "        col('emp1.emp_id').alias('Emp_id'),\n",
    "        col('emp1.name').alias('Emp_name'),\n",
    "        col('emp2.superior_emp_id').alias('Superior_id'),\n",
    "        col('emp2.name').alias('Superior_name')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a6ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.createOrReplaceTempView('employees')\n",
    "deptDF.createOrReplaceTempView('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9edc48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|dept_name|no_of_employees|\n",
      "+---------+---------------+\n",
      "|  Finance|              3|\n",
      "|Marketing|              1|\n",
      "|       IT|              1|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        dept_name,\n",
    "        COUNT(*) no_of_employees\n",
    "    FROM\n",
    "        employees e JOIN department d\n",
    "    ON\n",
    "        e.emp_dept_id = d.dept_id\n",
    "    GROUP BY\n",
    "        dept_name\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67612e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54e5a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(df2).show() # including duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "116c41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(df2).distinct().show() # excluding duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9680407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "| 34|James|\n",
      "| 45|Maria|\n",
      "| 45|  Jen|\n",
      "| 34| Jeff|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame df1 with columns name, and id\n",
    "data = [(\"James\",34), (\"Michael\",56), \\\n",
    "        (\"Robert\",30), (\"Maria\",24)\n",
    "       ]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "# Create DataFrame df2 with columns name and id\n",
    "# data2=[(34,\"James\", \"chacha\"),(45,\"Maria\", \"anty\"), \\\n",
    "#        (45,\"Jen\", \"khala\"),(34,\"Jeff\", \"phupo\")]\n",
    "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
    "       (45,\"Jen\"),(34,\"Jeff\")]\n",
    "# df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\", \"relation\"])\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ebf6a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "|  James| 34|\n",
      "|  Maria| 45|\n",
      "|    Jen| 45|\n",
      "|   Jeff| 34|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2).show() # allowMissingColumns=True in spark 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6afcaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Seqno: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce19b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|Seqno|<lambda>(Name)|\n",
      "+-----+--------------+\n",
      "|    1|    John jones|\n",
      "|    2|  Tracey smith|\n",
      "|    3|   Amy sanders|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_case = lambda str: str[0].upper() + str[1:]\n",
    "convertUdf = udf(convert_case,\n",
    "                 StringType())  # default return type is string type\n",
    "df.select(col('Seqno'), convertUdf(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b33971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------------+\n",
      "|Seqno|        Name|Name in (Upper Case)|\n",
      "+-----+------------+--------------------+\n",
      "|    1|  john jones|          JOHN JONES|\n",
      "|    2|tracey smith|        TRACEY SMITH|\n",
      "|    3| amy sanders|         AMY SANDERS|\n",
      "+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperUdf = udf(str.upper)\n",
    "df.withColumn('Name in (Upper Case)', upperUdf(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ca0656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------------+\n",
      "|Seqno|Name_in_title|Name_in_uppercase|\n",
      "+-----+-------------+-----------------+\n",
      "|    1|   John jones|       JOHN JONES|\n",
      "|    2| Tracey smith|     TRACEY SMITH|\n",
      "|    3|  Amy sanders|      AMY SANDERS|\n",
      "+-----+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_case = lambda str: str[0].upper() + str[1:]\n",
    "upper_case = lambda x: x.upper()\n",
    "spark.udf.register('convertUDF', convert_case)\n",
    "spark.udf.register('to_upper', upper_case)\n",
    "df.createOrReplaceTempView('customers')\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        Seqno,\n",
    "        convertUDF(name) Name_in_title,\n",
    "        to_upper(name) Name_in_uppercase\n",
    "    FROM\n",
    "        customers\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0620a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-----------------------+\n",
      "|Seqno|        Name|Every Letter Is Capital|\n",
      "+-----+------------+-----------------------+\n",
      "|    1|  john jones|             John Jones|\n",
      "|    2|tracey smith|           Tracey Smith|\n",
      "|    3| amy sanders|            Amy Sanders|\n",
      "+-----+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType())\n",
    "def convert_every_word_first_lettter_to_capital(s):\n",
    "    new_split = s.split(' ')\n",
    "    new_str = ''\n",
    "    return ' '.join([i[0].upper() + i[1:] for i in new_split])\n",
    "\n",
    "\n",
    "df.withColumn('Every Letter Is Capital',\n",
    "              convert_every_word_first_lettter_to_capital(col('name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ae39bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------------+\n",
      "|Seqno|Name_in_title|Name_in_uppercase|\n",
      "+-----+-------------+-----------------+\n",
      "|    1|   John jones|       JOHN JONES|\n",
      "+-----+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_case = lambda str: str[0].upper() + str[1:]\n",
    "upper_case = lambda x: x.upper()\n",
    "spark.udf.register('convertUDF', convert_case)\n",
    "spark.udf.register('to_upper', upper_case)\n",
    "df.createOrReplaceTempView('customers')\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        Seqno,\n",
    "        convertUDF(name) Name_in_title,\n",
    "        to_upper(name) Name_in_uppercase\n",
    "    FROM\n",
    "        customers\n",
    "    WHERE\n",
    "        name IS NOT null AND name like '%joh%'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c844427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input 'FROM' expecting <EOF>(line 6, pos 4)\\n\\n== SQL ==\\n\\n    SELECT\\n        Seqno,\\n        convertUDF(name) Name_in_title,\\n        to_upper(name) Name_in_uppercase\\n    FROM\\n----^^^\\n        NAME_TABLE2\\n    WHERE\\n        name IS NOT null AND name like '%joh%'.\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark-2.4.6-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.6-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'FROM' expecting <EOF>(line 6, pos 4)\n\n== SQL ==\n\n    SELECT\n        Seqno,\n        convertUDF(name) Name_in_title,\n        to_upper(name) Name_in_uppercase\n    FROM\n----^^^\n        NAME_TABLE2\n    WHERE\n        name IS NOT null AND name like '%joh%'.\n\r\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\r\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_324\\3368898119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mWHERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mname\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mnull\u001b[0m \u001b[0mAND\u001b[0m \u001b[0mname\u001b[0m \u001b[0mlike\u001b[0m \u001b[1;34m'%joh%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m ''').show() # gives error not handling null in udf\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.4.6-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \"\"\"\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.6-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.6-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParseException\u001b[0m: \"\\nmismatched input 'FROM' expecting <EOF>(line 6, pos 4)\\n\\n== SQL ==\\n\\n    SELECT\\n        Seqno,\\n        convertUDF(name) Name_in_title,\\n        to_upper(name) Name_in_uppercase\\n    FROM\\n----^^^\\n        NAME_TABLE2\\n    WHERE\\n        name IS NOT null AND name like '%joh%'.\\n\""
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        Seqno,\n",
    "        convertUDF(name) Name_in_title,\n",
    "        to_upper(name) Name_in_uppercase\n",
    "    FROM\n",
    "        NAME_TABLE2\n",
    "    WHERE\n",
    "        name IS NOT null AND name like '%joh%'.\n",
    "''').show() # gives error not handling null in udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b8e1e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9148\\241405265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# handling null in udf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconvert_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;34m'null'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'convertUDF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Seqno\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m data = [(\"1\", \"john jones\"),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# handling null in udf\n",
    "convert_case = lambda str:  'null' if str is None else str[0].upper() + str[1:] \n",
    "spark.udf.register('convertUDF', convert_case)\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE3\")\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        Seqno,\n",
    "        convertUDF(name) Name_in_title\n",
    "    FROM\n",
    "        NAME_TABLE3\n",
    "    WHERE\n",
    "        name IS NOT null AND convertUDF(name) like '%Joh%'\n",
    "''').show() # gives error not handling null in udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebf282e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d037fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PROJECT', 1),\n",
       " ('GUTENBERG’S', 1),\n",
       " ('ALICE’S', 1),\n",
       " ('ADVENTURES', 1),\n",
       " ('IN', 1),\n",
       " ('WONDERLAND', 1),\n",
       " ('PROJECT', 1),\n",
       " ('GUTENBERG’S', 1),\n",
       " ('ADVENTURES', 1),\n",
       " ('IN', 1),\n",
       " ('WONDERLAND', 1),\n",
       " ('PROJECT', 1),\n",
       " ('GUTENBERG’S', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.map(lambda x: x.upper())\n",
    "rdd = rdd.map(lambda x: (x, 1))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf59d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33cf0a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    JAMES Smith|  Male|        60|\n",
      "|      ANNA Rose|Female|        82|\n",
      "|ROBERT Williams|  Male|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = df.rdd.map(\n",
    "    lambda x: (f'{x[0].upper()} {x[1]}', 'Male' if x[2] == 'M' else 'Female', x[3] * 2)\n",
    ")\n",
    "rdd.toDF([\"name\",\"gender\",\"new_salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f9f9ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    JAMES Smith|  Male|        60|\n",
      "|      ANNA Rose|Female|        82|\n",
      "|ROBERT Williams|  Male|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = df.rdd.map(\n",
    "    lambda x: (f'{x[\"firstname\"].upper()} {x[\"lastname\"]}', 'Male' if x[\"gender\"] == 'M' else 'Female', x[\"salary\"] * 2)\n",
    ")\n",
    "rdd.toDF([\"name\",\"gender\",\"new_salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fad21241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James Smith|  Male|        60|\n",
      "|      Anna Rose|Female|        82|\n",
      "|Robert Williams|  Male|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_func(x):\n",
    "    name = x['firstname'] + ' ' + x['lastname']\n",
    "    gender = 'Male' if x[\"gender\"] == 'M' else 'Female'\n",
    "    salary = x[\"salary\"] * 2\n",
    "    return (name, gender, salary)\n",
    "rdd = df.rdd.map(lambda x: custom_func(x))\n",
    "rdd.toDF([\"name\",\"gender\",\"new_salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11504bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'Project Gutenberg’s',\n",
       " 'Adventures in Wonderland',\n",
       " 'Project Gutenberg’s']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ca65bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rdd.flatMap(lambda x: x.lower()).collect()[:10])\n",
    "rdd.flatMap(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c50e2ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|      name|knownLanguages|          properties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94c31118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n",
      "|     name|programming_languages|\n",
      "+---------+---------------------+\n",
      "|    James|                 Java|\n",
      "|    James|                Scala|\n",
      "|  Michael|                Spark|\n",
      "|  Michael|                 Java|\n",
      "|  Michael|                 null|\n",
      "|   Robert|               CSharp|\n",
      "|   Robert|                     |\n",
      "|Jefferson|                    1|\n",
      "|Jefferson|                    2|\n",
      "+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', explode(df.knownLanguages).alias('programming_languages')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7339faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a6291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e6256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_no(df):\n",
    "    print(df.Seqno)\n",
    "df.foreach(get_seq_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24acb5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda df: accum.add(int(df.Seqno)))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b323a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize(list(range(1, 4)))\n",
    "rdd.foreach(lambda r: accum.add(r))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11836d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=35), Row(id=47), Row(id=69), Row(id=87), Row(id=89)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1, 101)\n",
    "df.sample(fraction=0.06).collect() # this will not contain duplicates\n",
    "df.sample(fraction=0.06, seed=123).collect() # return the same sample every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7483e32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=23), Row(id=24), Row(id=25), Row(id=32), Row(id=58)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(withReplacement=True, fraction=0.06).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d1e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|null               |PR   |30100     |\n",
      "|2  |704    |null    |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |null    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|null               |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./zip_codes.csv', inferSchema=True, header=True)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948ebc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                 --|   PR|     30100|\n",
      "|  2|    704| unknown|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709| unknown|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                 --|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value=0).show()\n",
    "df.na.fill(value=0, subset=['population']).show()\n",
    "df.na.fill({'type': 'unknown', 'population': 0, 'city': '--'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b00aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd011995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|     0| 4000|     0|4000|\n",
      "|  Beans|     0| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|     0|1000|\n",
      "|Carrots|  2000| 1200|     0|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Product').pivot('Country').agg(sum('Amount').alias('Total Amount')).fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fec392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|Product|Canada| USA|\n",
      "+-------+------+----+\n",
      "| Orange|     0|4000|\n",
      "|  Beans|     0|1600|\n",
      "| Banana|  2000|1000|\n",
      "|Carrots|  2000|1500|\n",
      "+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# because of performance issue, you can also provide column values as an second argument\n",
    "columns = ['Canada', 'USA']\n",
    "df.groupBy('Product').pivot('Country', columns).agg(sum('Amount').alias('Total Amount')).fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35505b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|     0| 4000|     0|4000|\n",
      "|  Beans|     0| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|     0|1000|\n",
      "|Carrots|  2000| 1200|     0|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# to improve more performance, do it in 2 phases\n",
    "df.groupBy('Product', 'Country').sum('Amount').groupBy('Product').pivot(\n",
    "    'Country').sum('sum(Amount)').fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9522e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "| Orange|    USA| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "|  Beans|    USA| 1600|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "| Banana|    USA| 1000|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "|Carrots|    USA| 1500|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot = df.groupBy('Product', 'Country').sum('Amount').groupBy(\n",
    "    'Product').pivot('Country').sum('sum(Amount)')\n",
    "df_pivot.select(\n",
    "    'Product',\n",
    "    expr(\n",
    "        \"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico, 'USA', USA) AS (Country, Total)\" \n",
    "    )).where('Total IS NOT NULL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4ae3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./simle_zip_code.csv', inferSchema=True, header=True)\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b65de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header', True).partitionBy('state').mode('overwrite').csv('./tmp/zipcode_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a5d0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header', True).partitionBy('state', 'city').mode('overwrite').csv('./tmp/zipcode_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9631948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(2).write.option('header', True).partitionBy('state').mode('overwrite').csv('./tmp/zipcode_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ddeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('header', True).option('maxRecordsPerFile', 2).partitionBy('state').mode('overwrite').csv('./tmp/zipcode_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7429a4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|state|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read partition data\n",
    "parDF = spark.read.option('header', True).csv('./tmp/zipcode_state')\n",
    "parDF.createOrReplaceTempView('zipcode')\n",
    "spark.sql('''\n",
    "    SELECT * FROM zipcode\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51c5e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31174e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|employee_name|department|salary|rn |\n",
      "+-------------+----------+------+---+\n",
      "|James        |Sales     |3000  |1  |\n",
      "|James        |Sales     |3000  |2  |\n",
      "|Robert       |Sales     |4100  |3  |\n",
      "|Saif         |Sales     |4100  |4  |\n",
      "|Michael      |Sales     |4600  |5  |\n",
      "|Maria        |Finance   |3000  |1  |\n",
      "|Scott        |Finance   |3300  |2  |\n",
      "|Jen          |Finance   |3900  |3  |\n",
      "|Kumar        |Marketing |2000  |1  |\n",
      "|Jeff         |Marketing |3000  |2  |\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WindowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "df.withColumn('rn', row_number().over(WindowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "978924c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|employee_name|department|salary|rn |\n",
      "+-------------+----------+------+---+\n",
      "|James        |Sales     |3000  |1  |\n",
      "|James        |Sales     |3000  |1  |\n",
      "|Robert       |Sales     |4100  |3  |\n",
      "|Saif         |Sales     |4100  |3  |\n",
      "|Michael      |Sales     |4600  |5  |\n",
      "|Maria        |Finance   |3000  |1  |\n",
      "|Scott        |Finance   |3300  |2  |\n",
      "|Jen          |Finance   |3900  |3  |\n",
      "|Kumar        |Marketing |2000  |1  |\n",
      "|Jeff         |Marketing |3000  |2  |\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('rn', rank().over(WindowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6860da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+\n",
      "|employee_name|department|salary|rn |\n",
      "+-------------+----------+------+---+\n",
      "|James        |Sales     |3000  |1  |\n",
      "|James        |Sales     |3000  |1  |\n",
      "|Robert       |Sales     |4100  |2  |\n",
      "|Saif         |Sales     |4100  |2  |\n",
      "|Michael      |Sales     |4600  |3  |\n",
      "|Maria        |Finance   |3000  |1  |\n",
      "|Scott        |Finance   |3300  |2  |\n",
      "|Jen          |Finance   |3900  |3  |\n",
      "|Kumar        |Marketing |2000  |1  |\n",
      "|Jeff         |Marketing |3000  |2  |\n",
      "+-------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('rn', dense_rank().over(WindowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2710429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---------------+\n",
      "|employee_name|department|salary|Cummulative Sum|\n",
      "+-------------+----------+------+---------------+\n",
      "|James        |Sales     |3000  |6000           |\n",
      "|James        |Sales     |3000  |6000           |\n",
      "|Robert       |Sales     |4100  |14200          |\n",
      "|Saif         |Sales     |4100  |14200          |\n",
      "|Michael      |Sales     |4600  |18800          |\n",
      "|Maria        |Finance   |3000  |3000           |\n",
      "|Scott        |Finance   |3300  |6300           |\n",
      "|Jen          |Finance   |3900  |10200          |\n",
      "|Kumar        |Marketing |2000  |2000           |\n",
      "|Jeff         |Marketing |3000  |5000           |\n",
      "+-------------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WindowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "df.withColumn('Cummulative Sum', sum('salary').over(WindowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab84e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|Lead|\n",
      "+-------------+----------+------+----+\n",
      "|James        |Sales     |3000  |4100|\n",
      "|James        |Sales     |3000  |4100|\n",
      "|Robert       |Sales     |4100  |4600|\n",
      "|Saif         |Sales     |4100  |null|\n",
      "|Michael      |Sales     |4600  |null|\n",
      "|Maria        |Finance   |3000  |3900|\n",
      "|Scott        |Finance   |3300  |null|\n",
      "|Jen          |Finance   |3900  |null|\n",
      "|Kumar        |Marketing |2000  |null|\n",
      "|Jeff         |Marketing |3000  |null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WindowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "df.withColumn('Lead', lead('salary', 2).over(WindowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2b8d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f92e14de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2023-03-13|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias('current_date')).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "862968fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|date_format|\n",
      "+-----------+\n",
      "| 02-01-2020|\n",
      "| 03-01-2019|\n",
      "| 03-01-2021|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_format(col('input'), 'MM-dd-yyyy').alias('date_format')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3e3579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(to_date(col('input')).alias('to_date')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa538d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|difference|\n",
      "+----------+\n",
      "|      1136|\n",
      "|      1473|\n",
      "|       742|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(datediff(current_date(), col('input')).alias('difference')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1cae8a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26d0b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('value', from_json(df.value, MapType(StringType(), StringType())))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7d6334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               value|             to_json|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|[Zipcode -> 704, ...|{\"Zipcode\":\"704\",...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('to_json', to_json(col('value'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af2e3e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+-----+\n",
      "| id|Zipcode|ZipCodeType|       City|State|\n",
      "+---+-------+-----------+-----------+-----+\n",
      "|  1|    704|   STANDARD|PARC PARQUE|   PR|\n",
      "+---+-------+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id', json_tuple('value', 'Zipcode', 'ZipCodeType', 'City',\n",
    "                           'State')).toDF('id', 'Zipcode', 'ZipCodeType',\n",
    "                                          'City', 'State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd764bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|   HahaCity|\n",
      "+---+-----------+\n",
      "|  1|PARC PARQUE|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id', get_json_object(col('value'), '$.City').alias('HahaCity')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
